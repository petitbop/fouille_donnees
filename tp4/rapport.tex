\documentclass[a4paper, 12pt]{article}


\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[usenames,dvipsnames]{xcolor}


\setcounter{secnumdepth}{4}
% TAILLE DES PAGES (A4 serré)

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}
\setlength{\textwidth}{17cm}
\setlength{\textheight}{24cm}
\setlength{\oddsidemargin}{-.7cm}
\setlength{\evensidemargin}{-.7cm}
\setlength{\topmargin}{-.5in}

% Commandes de mise en page
\newcommand{\fichier}[1]{\emph{#1}}
\newcommand{\nom}[1]{\emph{#1}}
\newcommand{\Fig}[1]{Fig \ref{#1} p. \pageref{#1}}
\newcommand{\itemi}{\item[$\bullet$]}

% Commandes de maths
\newcommand{\fonction}[3]{#1 : #2 \to #3}
\newcommand{\intr}[2]{\left[ #1 ; #2 \right]}
\newcommand{\intn}[2]{\left[\![ #1 ; #2 \right]\!]}
\newcommand{\intro}[2]{\left] #1 ; #2 \right[}
\newcommand{\intrsod}[2]{\left[ #1 ; #2 \right[}
\newcommand{\ps}[2]{\langle #1, #2 \rangle}
\newcommand{\mdelta}[1]{\boldsymbol{\delta_{#1}}}
%% \newcommand{\mdelta}[1]{\delta_{\textbf{#1}}}

\pagenumbering{arabic}
\graphicspath{{images/}}

\title{Fouille de données TP4 : Classification documentaire} 
\author{Pierre Petitbon \and Florian Privé \and Xinrui Xu}
\date{}

\begin{document}

\maketitle

\begin{enumerate}
\setlength{\itemsep}{20pt}

\item[Q1)] 
Nous avons un fichier contenant, sur chaque ligne, un vecteur correspondant à chaque document. Chaque composante $i$ d'un vecteur correspond au nombre d'occurrence du $i^{ème}$ mot du vocabulaire. Pour optimiser l'espace de stockage, ce vecteur est codé dans le fichier sous la forme indice-valeur qui consiste à obtenir une représentation compacte des vecteurs de documents en ne codant que les mots qui sont présents dans le document ainsi que le nombre d'occurrence associé à ce mot. 

Ainsi, pour trouver la taille du vocabulaire, il faut écrire un parser qui lit les indices présents dans chaque vecteur et trouver le maximum de ces indices. Le plus grand des indices correspond au nombre de mots différents qui sont présents dans l'ensemble des documents de la base. On obtient ainsi la taille du vocabulaire : 
\begin{math}|V| = 141144 \end{math}
\\
Le premier nombre de chaque ligne correspond à la classe à laquelle appartient ce document. Pour trouver le nombre de documents dans une classe $k$, il suffit de compter le nombre de lignes commençant par $k$. On obtient ainsi : 
\\
\\
\begin{center}
\begin{tabular}{|l|c|r|}
	\hline
   classe & nombre de documents \\
   \hline
   1 & 5894 \\
   \hline
   2 & 1003 \\
   \hline
   3 & 2472 \\
   \hline
   4 & 2207 \\
   \hline
   5 & 6010 \\
   \hline
   6 & 2992 \\
   \hline
   7 & 1586 \\
   \hline
   8 & 1226 \\
   \hline
   9 & 2007 \\
   \hline
   10 & 3982 \\
   \hline
   11 & 7757 \\
   \hline
   12 & 3644 \\
   \hline
   13 & 3405 \\
   \hline
   14 & 2307 \\
   \hline
   15 & 1040 \\
   \hline
   16 & 1460 \\
   \hline
   17 & 1191 \\
   \hline
   18 & 1733 \\
   \hline
   19 & 4745 \\
   \hline
   20 & 1411 \\
   \hline
   21 & 1016 \\
   \hline
   22 & 3018 \\
   \hline
    \end{tabular}
   
  \begin{tabular}{|l|c|r|}
  \hline
  classe & nombre de documents \\
  \hline
   23 & 1050 \\
   \hline
   24 & 1184 \\
   \hline
   25 & 1624 \\
   \hline
   26 & 1296 \\
   \hline
   27 & 1018 \\
   \hline
   28 & 1049 \\
   \hline
   29 & 1376 \\
   \hline
\end{tabular}
\end{center}


\item[Q2)] 
On veut scinder aléatoirement les $70703$ documents en deux ensembles :
\begin{itemize}
\item base d'entraînement ($52500$ documents) 
\item base de test ($18203$ documents)
\\
\\
Soit $ratio = \frac{52500}{70703}$ \\
On va tirer, en suivant une lui uniforme[0,1], une valeur $r$, pour chaque document $di$ :
\item Si $r<ratio$ on place le document $di$ dans la base apprentissage.
\item Sinon on le place dans la base test.
\end{itemize}
Ainsi la taille de la base d'apprentissage suit une loi binômiale de paramètres $n=70703$ et $p=ratio$. L'espérance de cette loi est de $n*p=52500$ et son écart-type est de $sqrt{n*p*(1-p)}=116$.
Vu la taille du problème, la différences entre la taille de la base d'apprentissage obtenue et celle voulue est négligeable.

\item[Q3)]
\begin{itemize}

\item Le modèle de Bernouilli : \\ 

Le modèle a pour paramètres (apprentissage) :\\
\begin{math}
   \hat{\theta}_{t_{i}|k}=\frac{df_{t_{i}}(k)+1}{N_{k}(S)+2} $(lissage de Laplace pour ne que $\hat{\theta}_{t_{i}|k} $ soit non nul)$\\
   \hat{\pi}_{k} = \frac{N_{k}(S)}{m}
\end{math}

La prédiction :\\
\begin{math}
   k(d^{'})=argmax\limits_{ k \in [1..K] } ( ln( \hat{\pi}_{k} ) + \sum\limits_{ t_{i} \in d^{'} } { ln(\hat{\theta}_{t_{i}|k}) } + \sum\limits_{ t_{i} \notin d^{'} } { ln(1- \hat{\theta}_{ t_{i} | k } )  } )
\end{math}
\\
\\
\\
\item Le modèle multinomial : \\  

Le modèle a pour paramètres (apprentissage) :\\
\begin{math}
   \hat{\theta}_{t_{i}|k}=\frac{ \sum\limits_{d\in S_k}{ tf_{t_{i} , d} + 1 } }{ \sum \limits_{1}^{V} { \sum \limits_{d \in S_{k}} {tf_{t_{i}, d} + V } } } \\
   \hat{\pi}_{k} = \frac{N_{k}(S)}{m}
\end{math}  

La prédiction :\\
\begin{math}
   k(d^{'})=argmax\limits_{ k \in [1..K] } ( ln(\hat{\pi}_{k}) + \sum\limits_{ t_{i} \in d^{'} } { w_{id^{'}}ln(\hat{\theta}_{t_{i}|k}) } )
\end{math}   
\\
\\
\\
\\
Nous avons codé les formules du modèle Bernouilli et du modèle multinomial pour la phase d'apprentissage et la phase de prédiction. Nous avons modifié quelques formules du modèle Bernouilli afin d'optimiser le code. 
\item Lors de la phase d'apprentissagede Bernouilli, nous avons choisi de calculer seulement les $df_{t_{i}}(k)$ et les $N_{k}(S)$ au lieu des $\hat{\theta}_{t_{i}|k}$ et des $\hat{\pi}_{k}$ pour ne stocker que des uint16\_t au lieu des doubles (ce qui prendrait beaucoup plus de mémoire).
\item Lors de la phase de prédiction de Bernouilli, nous n'avons pas codé la formule exactement. Nous avons calculé 
$ ln( \hat{\pi}_{k} ) + \sum\limits_{ t_{i} \in Vocab } { ln(1-\hat{\theta}_{t_{i}|k}) } + \sum\limits_{ t_{i} \notin d^{'} } ({ ln( \hat{\theta}_{ t_{i} | k } )-ln( 1-\hat{\theta}_{ t_{i} | k } ) }) $ de façon à minimiser le nombre de calculs de $log$ qui ralentirait beaucoup le programme.
\end{itemize}

\item[Q4)]
Avec le modèle Bernouilli : Taux de bonne classification $= 55\%$.\\
Avec le modèle Multinomiale : Taux de bonne classification $= 75\%$.\\

\item[Q5)]
A l'issue de 20 expériences, nous calculons la moyenne, la variance et l'écart-type des taux de bonne classification : \\
\begin{itemize}
\item Avec le modèle de Bernouilli : La moyenne vaut : $54.814625\%$. La variance vaut : $0.082031$. L'ecart-type vaut : $0.286411$.
\item Avec le modèle Multinomial : La moyenne vaut : $74.771996\%$. La variance vaut : $0.085938$. L'ecart-type vaut : $0.293151$.
\end{itemize}
\end{enumerate}

Conclusion : \\
Avec les deux modèles, la variance et l'écart-type des taux de bonne classification restent plutôt faible, donc les résultats des deux modèles sont très stables. La moyenne sur les taux de bonne classification avec les deux modèles sont assez différents. Le modèle Multinomial présente un taux de bonne classification largement supérieur à celui du modèle de Bernouilli (différence de $20\%$). Le modèle Multinomial est donc meilleur. Cela peut s'expliquer par le fait que le modèle de Bernouilli ne prend que en compte l'absence ou la présence d'un mot dans un document, alors que le modèle Multinomial prend également en compte la fréquence d'apparition d'un mot dans un document. 
\end{document}


